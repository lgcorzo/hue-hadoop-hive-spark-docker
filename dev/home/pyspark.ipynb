{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5fab7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc96680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.conda/envs/pyspark_code_env/lib/python3.8/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyter/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyter/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bb4b6c0f-ddc4-42ee-b9ef-f9a9887ddb94;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 163ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bb4b6c0f-ddc4-42ee-b9ef-f9a9887ddb94\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/16 09:34:33 WARN Client: Same path resource file:///home/jupyter/.ivy2/jars/io.delta_delta-core_2.12-2.3.0.jar added multiple times to distributed cache.\n",
      "23/08/16 09:34:33 WARN Client: Same path resource file:///home/jupyter/.ivy2/jars/io.delta_delta-storage-2.3.0.jar added multiple times to distributed cache.\n",
      "23/08/16 09:34:33 WARN Client: Same path resource file:///home/jupyter/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# Databricks notebook source\n",
    "from multiprocessing import current_process\n",
    "import glob\n",
    "import pyspark.pandas as pf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "conf.set(\"spark.driver.memory\", \"8g\") \n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"dev_test_notebook\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(conf=conf) \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'file:/landing/Raw/Merlin/ecb2ef6fdf3042339961920fee90b950/ecb2ef6fdf3042339961920fee90b950/DIS_NEST_NEST_00000300_0.parquet'\n",
    "read_local_file_pd = pd.read_parquet(file_path, engine='fastparquet')\n",
    "read_local_file_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00333d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_local_file_pf = pd.read_parquet(file_path, engine='fastparquet')\n",
    "pf_df = pf.DataFrame(read_local_file_pf)\n",
    "pf_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb099cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false)) error en los parquet\n",
    "read_local_file_pf = pf.read_parquet(file_path,engine='fastparquet')\n",
    "read_local_file_pf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be58876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file in the hdbs if is local file:/\n",
    "file_path = 'file:/landing/Staging/Merlin/Smartquoting/3cb9a027-3b3e-4f3d-b9df-d8478cf812c8/8abe3f95-37f1-4cbc-a152-52e79ed26a65/staging_db_data_production.csv'\n",
    "read_local_file_pf = pf.read_csv(file_path,sep=\";\",\n",
    "                         decimal=',',\n",
    "                         encoding='ISO-8859-1',\n",
    "                         quotechar='\"',\n",
    "                         engine='python',\n",
    "                         dtype=str,\n",
    "                         na_values='nan',\n",
    "                         inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_local_file_pf = read_local_file_pf[[\"Part_IsAssembly\", \"Part_Weight\", \"Part_RealArea\", \"Part_ExternalArea\", \"Part_Thickness\", \"Part_Length\", \"Part_RectangularArea\",\n",
    "            \"Part_IsFillerPart\", \"Part_MaterialDensity\", \"Part_RectangularWeight\", \"Part_MarkingPerimeter\",\"Part_ExternalWeight\", \"Part_CutPerimeter\",\n",
    "            \"Part_Width\", \"ManufacturingOrder_Quantity\", \"Part_WorkCenter\", \"Part_Material\", \"Part_Rotations\", \"Part_CreationMethod\", \"Tenant_ID\",\n",
    "            \"Plant_ID\"]]\n",
    "read_local_file_spf = read_local_file_pf.to_spark()\n",
    "display(read_local_file_spf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af218eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a Delta table in Hive\n",
    "hive_table_name = 'default.cr_table_demo_delta'\n",
    "hive_table_path= f'/user/jupyter/result/{hive_table_name}'\n",
    "\n",
    "# write table \n",
    "read_local_file_spf.write.format(\"delta\").mode('overwrite').save(hive_table_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the schema of the DataFrame\n",
    "schema = read_local_file_spf.schema\n",
    "# Create the table in Hive\n",
    "column_definitions = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in schema])\n",
    "print(column_definitions)\n",
    "# Create an external Hive table pointing to the Delta table location\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS {hive_table_name} ({column_definitions}) USING delta LOCATION '{hive_table_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ea42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an external Hive table pointing to the Delta table location\n",
    "hive_table_name = 'default.cr_table_demo_hive'\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS {hive_table_name} ({column_definitions}) USING hive LOCATION '{hive_table_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023ffd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/16 09:34:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive_table_name = 'default.sklearn_housing'\n",
    "len(spark.sql(f\"select * from {hive_table_name}\").tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35ff53b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
