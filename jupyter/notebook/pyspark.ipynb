{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://livy.apache.org/examples/\n",
    "import json, pprint, requests, textwrap\n",
    "host = 'http://master:8998'\n",
    "data = {'kind': 'spark'}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_url = host + r.headers['location']\n",
    "r = requests.get(session_url, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_url = session_url + '/statements'\n",
    "data = {'code': '1 + 1'}\n",
    "r = requests.post(statements_url, data=json.dumps(data), headers=headers)\n",
    "r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399482fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyter/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyter/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d163c74f-5089-4e05-aab4-a1f52e45a1e5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 417ms :: artifacts dl 28ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d163c74f-5089-4e05-aab4-a1f52e45a1e5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/17ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/08 18:25:35 WARN Client: Same path resource file:///home/jupyter/.ivy2/jars/io.delta_delta-core_2.12-2.4.0.jar added multiple times to distributed cache.\n",
      "23/11/08 18:25:35 WARN Client: Same path resource file:///home/jupyter/.ivy2/jars/io.delta_delta-storage-2.4.0.jar added multiple times to distributed cache.\n",
      "23/11/08 18:25:35 WARN Client: Same path resource file:///home/jupyter/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "from pyspark.conf import SparkConf\n",
    "from delta import configure_spark_with_delta_pip\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "conf.set(\"spark.driver.memory\", \"8g\") \n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "builder = SparkSession.builder.appName(\"dev_test_notebook_2\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .config(conf=conf)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fad11f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 07:56:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/08 07:56:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/08 07:56:55 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/08 07:56:55 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/08 07:56:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/08 07:56:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/08 07:56:55 WARN HiveMetaStore: Location: hdfs://master/user/hive/warehouse/src_test2 specified for non-external table:src_test2\n",
      "23/11/08 07:56:56 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src_test2 (key INT, value STRING) USING hive\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH '/opt/spark/examples/src/main/resources/kv1.txt' INTO TABLE src_test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db995bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM src_test2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d88f94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     500|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM src_test2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e1ccf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT key, value FROM src_test2 WHERE key < 10 ORDER BY key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c682d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
    "for record in stringsDS.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987c966",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"key\", \"value\")\n",
    "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "recordsDF.createOrReplaceTempView(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981c7d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM records r JOIN src_test2 s ON r.key = s.key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM default.cr_table_demo_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ff53b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
